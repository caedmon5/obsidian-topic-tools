#!/usr/bin/env python3
"""
instantiate_topic_stubs.py

Script 1: Extract Obsidian wikilinks from a vault and instantiate missing topic-note stub files.

Key properties:
- Dry-run by default (no filesystem changes unless --apply)
- Never overwrites existing files
- Skips fenced code blocks and inline code
- Extracts canonical page names from [[Page]], [[Page|Alias]], [[Page#Heading]]
- Writes audit reports (CSV + JSONL + sorted TXT) for downstream cleaning/consolidation (Script 2)
"""

from __future__ import annotations

import argparse
import csv
import json
import os
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, Iterator, List, Optional, Set, Tuple


WIKILINK_RE = re.compile(
    r"(?<!!)\[\[([^\]\|#]+)(?:#[^\]]*)?(?:\|[^\]]*)?\]\]"
)


@dataclass(frozen=True)
class LinkFinding:
    topic: str
    source_file: str
    line_no: int


def utc_now_stamp() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")


def remove_inline_code(line: str) -> str:
    """
    Remove inline code spans delimited by single backticks.
    This is a conservative parser: it toggles on each backtick.
    """
    out: List[str] = []
    in_code = False
    for ch in line:
        if ch == "`":
            in_code = not in_code
            continue
        if not in_code:
            out.append(ch)
    return "".join(out)


def iter_markdown_files(vault_root: Path, exclude_dirs: Set[Path]) -> Iterator[Path]:
    for p in vault_root.rglob("*.md"):
        # Exclude by directory ancestry
        if any(excl in p.parents for excl in exclude_dirs):
            continue
        yield p

def build_vault_basename_index(
    vault_root: Path, exclude_dirs: Set[Path]
) -> dict[str, List[Path]]:
    """
    Map basename-without-extension -> list of matching markdown paths in the vault.
    Used to determine whether [[Target]] is already instantiated anywhere.
    """
    idx: dict[str, List[Path]] = {}
    for p in iter_markdown_files(vault_root, exclude_dirs=exclude_dirs):
        stem = p.stem  # filename without .md
        idx.setdefault(stem, []).append(p)
    return idx

def extract_topics_from_file(md_path: Path) -> Iterator[Tuple[str, int]]:
    """
    Yield (topic, line_no) from a markdown file, skipping fenced code blocks and inline code.
    """
    in_fenced = False
    fence_pat = re.compile(r"^\s*(```|~~~)")

    try:
        with md_path.open("r", encoding="utf-8") as f:
            for i, raw in enumerate(f, start=1):
                line = raw.rstrip("\n")

                # Toggle fenced-code state
                if fence_pat.match(line):
                    in_fenced = not in_fenced
                    continue

                if in_fenced:
                    continue

                cleaned = remove_inline_code(line)
                for m in WIKILINK_RE.finditer(cleaned):
                    topic = m.group(1).strip()
                    if topic:
                        yield (topic, i)
    except UnicodeDecodeError:
        # If something is not UTF-8, skip and let the report show it was ignored.
        return


def is_valid_topic_filename(topic: str) -> Tuple[bool, Optional[str]]:
    """
    Determine whether a topic can be safely used as a filename on Linux.
    We reject path separators and NUL; we also reject empty/whitespace-only.
    """
    t = topic.strip()
    if not t:
        return False, "empty topic after stripping whitespace"
    if "\x00" in t:
        return False, "contains NUL character"
    if "/" in t:
        return False, "contains '/' which would create subpaths"
    return True, None


def topic_to_path(topics_dir: Path, topic: str) -> Path:
    return topics_dir / f"{topic}.md"


def build_stub_content(topic: str) -> str:
    # Minimal, stable frontmatter scaffolding consistent with your conventions.
    return (
        "---\n"
        f"title: \"{topic}\"\n"
        "type: \"Topic\"\n"
        "autoupdate: false\n"
        "related files:\n"
        "  # - \"[[Some Related Note]]\"\n"
        "---\n\n"
        "<!-- This is an autogenerated topic stub. Flesh out as needed. -->\n"
    )


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def write_reports(
    report_dir: Path,
    stamp: str,
    findings: List[LinkFinding],
    unique_topics: List[str],
    actions: List[dict],
) -> None:
    ensure_dir(report_dir)

    # 1) Sorted list (easy to diff/scan)
    sorted_txt = report_dir / f"all_topics_sorted_{stamp}.txt"
    with sorted_txt.open("w", encoding="utf-8") as f:
        for t in sorted(unique_topics, key=lambda s: s.casefold()):
            f.write(t + "\n")

    # 2) Findings (where topics appear) as JSONL
    findings_jsonl = report_dir / f"topic_findings_{stamp}.jsonl"
    with findings_jsonl.open("w", encoding="utf-8") as f:
        for fd in findings:
            f.write(
                json.dumps(
                    {
                        "topic": fd.topic,
                        "source_file": fd.source_file,
                        "line_no": fd.line_no,
                    },
                    ensure_ascii=False,
                )
                + "\n"
            )

    # 3) Actions taken / planned as CSV + JSONL
    actions_csv = report_dir / f"topic_actions_{stamp}.csv"
    with actions_csv.open("w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["topic", "action", "path", "reason"])
        w.writeheader()
        for a in actions:
            w.writerow(
                {
                    "topic": a.get("topic", ""),
                    "action": a.get("action", ""),
                    "path": a.get("path", ""),
                    "reason": a.get("reason", ""),
                }
            )

    actions_jsonl = report_dir / f"topic_actions_{stamp}.jsonl"
    with actions_jsonl.open("w", encoding="utf-8") as f:
        for a in actions:
            f.write(json.dumps(a, ensure_ascii=False) + "\n")


def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(
        description="Extract Obsidian wikilinks and instantiate missing topic-note stubs."
    )
    ap.add_argument("--vault", required=True, help="Path to vault root")
    ap.add_argument("--topics", required=True, help="Path to Topic Notes folder")
    ap.add_argument(
        "--apply",
        action="store_true",
        help="Actually create stub files (default: dry-run)",
    )
    ap.add_argument(
        "--report-dir",
        default=".",
        help="Directory for audit reports (default: current directory)",
    )
    ap.add_argument(
        "--exclude-dir",
        action="append",
        default=[],
        help="Directory to exclude (repeatable, relative to vault or absolute)",
    )
    ap.add_argument(
        "--max-create",
        type=int,
        default=0,
        help="Cap number of stubs created in one run (0 = no cap)",
    )
    return ap.parse_args()


def main() -> int:
    args = parse_args()
    vault_root = Path(args.vault).expanduser().resolve()
    topics_dir = Path(args.topics).expanduser().resolve()
    report_dir = Path(args.report_dir).expanduser().resolve()

    if not vault_root.exists() or not vault_root.is_dir():
        raise SystemExit(f"--vault does not exist or is not a directory: {vault_root}")
    if not topics_dir.exists() or not topics_dir.is_dir():
        raise SystemExit(f"--topics does not exist or is not a directory: {topics_dir}")

    # Exclusions
    exclude_dirs: Set[Path] = set()
    # Always exclude the Topic Notes folder itself from scanning by default to avoid self-noise.
    exclude_dirs.add(topics_dir)

    for ex in args.exclude_dir:
        p = Path(ex).expanduser()
        if not p.is_absolute():
            p = (vault_root / p)
        exclude_dirs.add(p.resolve())

    # Vault-wide basename index (stem -> paths).
    # NOTE: this must include *all* folders (including Topic Notes) because we use it
    # to determine whether a target filename already exists anywhere in the vault.
    vault_basename_index = build_vault_basename_index(vault_root)

    stamp = utc_now_stamp()

    findings: List[LinkFinding] = []
    topics_set: Set[str] = set()

    scanned = 0
    skipped_non_utf8 = 0

    for md in iter_markdown_files(vault_root, exclude_dirs=exclude_dirs):
        scanned += 1
        try:
            any_yielded = False
            for topic, line_no in extract_topics_from_file(md):
                any_yielded = True
                topics_set.add(topic)
                findings.append(
                    LinkFinding(topic=topic, source_file=str(md), line_no=line_no)
                )
            # If file couldnâ€™t be decoded, extract_topics_from_file returns nothing.
            # We can only detect non-UTF8 by trying to open; done inside extractor.
        except UnicodeDecodeError:
            skipped_non_utf8 += 1
            continue

    unique_topics = sorted(topics_set, key=lambda s: s.casefold())

    # Plan actions
    actions: List[dict] = []
    to_create: List[Tuple[str, Path]] = []
    invalid: List[Tuple[str, str]] = []

    for t in unique_topics:
        ok, reason = is_valid_topic_filename(t)
        if not ok:
            invalid.append((t, reason or "invalid filename"))
            actions.append(
                {
                    "topic": t,
                    "action": "skip_invalid",
                    "path": "",
                    "reason": reason or "invalid filename",
                }
            )
            continue

        # Does '<topic>.md' already exist anywhere in the vault?
        matches = vault_basename_index.get(t, [])
        if matches:
            if len(matches) == 1:
                actions.append(
                    {
                        "topic": t,
                        "action": "instantiated",
                        "path": str(matches[0]),
                        "reason": "filename exists in vault",
                    }
                )
            else:
                actions.append(
                    {
                        "topic": t,
                        "action": "instantiated_duplicate",
                        "path": " | ".join(str(p) for p in matches),
                        "reason": "multiple files share this basename in vault",
                    }
                )
            continue

        # Not instantiated anywhere: plan/perform creation in Topic Notes/
        target = topic_to_path(topics_dir, t)
        to_create.append((t, target))
        actions.append(
            {
                "topic": t,
                "action": "create" if args.apply else "create_planned",
                "path": str(target),
                "reason": "missing file in vault",
            }
        )

    # Apply changes if requested
    created = 0
    if args.apply:
        ensure_dir(topics_dir)
        cap = args.max_create if args.max_create and args.max_create > 0 else None

        for (t, target) in to_create:
            if cap is not None and created >= cap:
                break
            if target.exists():
                continue  # idempotency guard
            content = build_stub_content(t)
            target.write_text(content, encoding="utf-8")
            created += 1

    # Reports
    write_reports(
        report_dir=report_dir,
        stamp=stamp,
        findings=findings,
        unique_topics=unique_topics,
        actions=actions,
    )

    # Console summary
    planned = len(to_create)
    instantiated = sum(1 for a in actions if a["action"] in ("instantiated", "instantiated_duplicate"))
    invalid_n = len(invalid)

    mode = "APPLY" if args.apply else "DRY-RUN"
    print(f"[{mode}] Vault scanned: {scanned} markdown files")
    print(f"[{mode}] Unique topics found: {len(unique_topics)}")
    print(f"[{mode}] Instantiated targets (anywhere in vault): {instantiated}")
    print(f"[{mode}] Invalid topics skipped: {invalid_n}")
    if args.apply:
        print(f"[{mode}] Topic stubs created: {created}")
        if args.max_create and created < planned:
            print(f"[{mode}] Creation capped by --max-create={args.max_create}")
    else:
        print(f"[{mode}] Topic stubs to create: {planned}")

    print(f"[{mode}] Reports written to: {report_dir}")
    print(f"[{mode}] Report stamp: {stamp}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
